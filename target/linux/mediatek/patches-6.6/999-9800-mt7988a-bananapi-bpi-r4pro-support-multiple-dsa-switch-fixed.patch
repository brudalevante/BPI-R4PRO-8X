diff -urN a/drivers/net/ethernet/mediatek/mtk_eth_soc.c b/drivers/net/ethernet/mediatek/mtk_eth_soc.c
--- a/drivers/net/ethernet/mediatek/mtk_eth_soc.c	2025-11-15 04:06:05.000000000 +0000
+++ b/drivers/net/ethernet/mediatek/mtk_eth_soc.c	2025-11-15 04:21:49.000000000 +0000
@@ -1047,7 +1047,8 @@
 		eth->qdma_shaper.speed[idx] = speed;
 
 out:
-	ofs = MTK_QTX_OFFSET * idx;
+	mtk_w32(eth, (idx / MTK_QTX_PER_PAGE), soc->reg_map->qdma.page);
+	ofs = MTK_QTX_OFFSET * (idx % MTK_QTX_PER_PAGE);
 	mtk_w32(eth, val, soc->reg_map->qdma.qtx_sch + ofs);
 }
 
@@ -2992,6 +2993,8 @@
 		mtk_w32(eth, ring->last_free_ptr, soc->reg_map->qdma.drx_ptr);
 
 		for (i = 0, ofs = 0; i < MTK_QDMA_NUM_QUEUES; i++) {
+			mtk_w32(eth, (i / MTK_QTX_PER_PAGE), soc->reg_map->qdma.page);
+
 			val = (QDMA_RES_THRES << 8) | QDMA_RES_THRES;
 			mtk_w32(eth, val, soc->reg_map->qdma.qtx_cfg + ofs);
 
@@ -3009,7 +3012,7 @@
 			if (mtk_is_netsys_v1(eth))
 				val |= MTK_QTX_SCH_LEAKY_BUCKET_EN;
 			mtk_w32(eth, val, soc->reg_map->qdma.qtx_sch + ofs);
-			ofs += MTK_QTX_OFFSET;
+			ofs = (ofs + MTK_QTX_OFFSET) % (MTK_QTX_OFFSET * MTK_QTX_PER_PAGE);
 		}
 		val = MTK_QDMA_TX_SCH_MAX_WFQ | (MTK_QDMA_TX_SCH_MAX_WFQ << 16);
 		mtk_w32(eth, val, soc->reg_map->qdma.tx_sch_rate);
@@ -4158,6 +4161,51 @@
 #endif
 }
 
+static void mtk_eth_set_dsa_pppq_offset(struct mtk_eth *eth)
+{
+       struct net_device *dev;
+       int i, offset = 0;
+
+       for (i = 0; i < MTK_MAX_DEVS; i++) {
+               dev = eth->netdev[i];
+               if (dev && netdev_uses_dsa(dev)) {
+                       eth->pppq_ofs[i] = offset;
+                       /* add the number of DSA user ports in that switch */
+                       offset += hweight32(dsa_user_ports(dev->dsa_ptr->ds));
+               }
+       }
+
+       /* save the total number of DSA user ports in the all switches */
+       eth->pppq_ofs[MTK_MAX_DEVS] = offset;
+}
+
+static void mtk_eth_set_dsa_user_idx(struct mtk_eth *eth, struct net_device *dev)
+{
+       const struct dsa_port *dp;
+       struct dsa_switch *ds;
+       struct mtk_mac *mac;
+       int p, cnt = 0;
+
+       if (!dsa_slave_dev_check(dev))
+               return;
+
+       dp = dsa_port_from_netdev(dev);
+       ds = dp->cpu_dp->ds;
+       mac = netdev_priv(dp->cpu_dp->master);
+
+       for (p = 0; p < ds->num_ports; p++) {
+               /* only handle user ports */
+               if (!dsa_is_user_port(ds, p))
+                       continue;
+
+               dp = dsa_to_port(ds, p);
+               if (dp->index < ARRAY_SIZE(mac->dsa_user_idx))
+                      mac->dsa_user_idx[dp->index] = cnt;
+
+               cnt++;
+       }
+}
+
 static int mtk_device_event(struct notifier_block *n, unsigned long event, void *ptr)
 {
 	struct mtk_mac *mac = container_of(n, struct mtk_mac, device_notifier);
@@ -4168,6 +4216,7 @@
 	struct list_head *iter;
 	struct dsa_port *dp;
 	unsigned int queue;
+	int offset;
 
 	if (event != NETDEV_CHANGE || dev->priv_flags & IFF_EBRIDGE)
 		return NOTIFY_DONE;
@@ -4192,8 +4241,14 @@
 		return NOTIFY_DONE;
 
 	if (dsa_slave_dev_check(dev)) {
+               	/* set txq offset when multiple dsa switch exist */
+               	mtk_eth_set_dsa_pppq_offset(eth);
+               	/* set DSA user port ordering index due to non-contiguous dp index */
+               	mtk_eth_set_dsa_user_idx(eth, dev);
 		dp = dsa_port_from_netdev(dev);
-		queue = dp->index + 3;
+                offset = eth->pppq_ofs[mac->id];
+                if (dp->index < ARRAY_SIZE(mac->dsa_user_idx))
+                      	queue = MTK_MAX_DEVS + offset + mac->dsa_user_idx[dp->index];
 	} else
 		queue = mac->id;
 
@@ -4597,7 +4652,7 @@
 	mtk_hw_dump_reg(eth, "ADMA", reg_map->pdma.rx_ptr, 0x300);
 	if (MTK_HAS_CAPS(eth->soc->caps, MTK_QDMA)) {
 		for (id = 0; id < MTK_QDMA_NUM_QUEUES / 16; id++) {
-			mtk_w32(eth, id, reg_map->qdma.page);
+			mtk_w32(eth, (id / MTK_QTX_PER_PAGE), reg_map->qdma.page);
 			pr_info("\nQDMA PAGE:%x ", mtk_r32(eth, reg_map->qdma.page));
 			mtk_hw_dump_reg(eth, "QDMA", reg_map->qdma.qtx_cfg, 0x100);
 			mtk_w32(eth, 0, reg_map->qdma.page);
@@ -5876,10 +5931,15 @@
 			    struct net_device *sb_dev)
 {
 	struct mtk_mac *mac = netdev_priv(dev);
+	struct mtk_eth *eth = mac->hw;
 	unsigned int queue = 0;
+	unsigned int dp_index = skb_get_queue_mapping(skb);
 
-	if (netdev_uses_dsa(dev))
-		queue = skb_get_queue_mapping(skb) + 3;
+       	if (skb->mark > 0 && skb->mark < dev->num_tx_queues)
+               	return skb->mark;
+	
+       	if (netdev_uses_dsa(dev) && dp_index < ARRAY_SIZE(mac->dsa_user_idx))
+ 		queue = MTK_MAX_DEVS + eth->pppq_ofs[mac->id] + mac->dsa_user_idx[dp_index];
 	else
 		queue = mac->id;
 
diff -urN a/drivers/net/ethernet/mediatek/mtk_eth_soc.h b/drivers/net/ethernet/mediatek/mtk_eth_soc.h
--- a/drivers/net/ethernet/mediatek/mtk_eth_soc.h	2025-11-15 04:06:09.000000000 +0000
+++ b/drivers/net/ethernet/mediatek/mtk_eth_soc.h	2025-11-25 05:28:27.000000000 +0000
@@ -26,7 +26,7 @@
 #define MTK_MAX_DSA_PORTS	7
 #define MTK_DSA_PORT_MASK	GENMASK(2, 0)
 
-#define MTK_QDMA_NUM_QUEUES	16
+#define MTK_QDMA_NUM_QUEUES	32
 #define MTK_QDMA_QUEUE_MASK	((1ULL << MTK_QDMA_NUM_QUEUES) - 1)
 #define MTK_QDMA_PAGE_SIZE	2048
 #define MTK_MAX_RX_LENGTH	1536
@@ -1647,6 +1647,7 @@
  * @pending_work:	The workqueue used to reset the dma ring
  * @state:		Initialization and runtime state of the device
  * @soc:		Holding specific data among vaious SoCs
+ * @pppq_ofs:          Offset of DSA user port in qdma PPPQ mapping
  */
 
 struct mtk_eth {
@@ -1667,6 +1668,7 @@
 	int				irq_fe[MTK_FE_IRQ_NUM];
 	int				irq_pdma[MTK_PDMA_IRQ_NUM];
 	u32				msg_enable;
+	u32                             pppq_ofs[MTK_MAX_DEVS];
 	unsigned long			sysclk;
 	struct regmap			*ethsys;
 	struct regmap			*infra;
@@ -1746,6 +1748,7 @@
  * @of_node:		Our devicetree node
  * @hw:			Backpointer to our main datastruture
  * @hw_stats:		Packet statistics counter
+ * @dsa_user_idx       DSA user port ordering index for qdma PPPQ mapping
  */
 struct mtk_mac {
 	int				id;
@@ -1765,6 +1768,7 @@
 	bool				tx_lpi_enabled;
 	u32				tx_lpi_timer;
 	struct notifier_block		device_notifier;
+	u8                              dsa_user_idx[16];
 };
 
 /* struct mtk_mux_data -	the structure that holds the private data about the
@@ -1921,14 +1925,9 @@
 }
 
 static inline int
-mtk_ppe_check_pppq_path(struct mtk_mac *mac, struct mtk_foe_entry *foe, int dsa_port)
+mtk_ppe_check_pppq_path(struct mtk_mac *mac, struct net_device *idev, int dsa_port)
 {
-	u32 sp = mtk_get_ib1_sp(mac->hw, foe);
-	bool wifi_rx;
-
-	wifi_rx = (sp == PSE_WDMA0_PORT ||
-		   sp == PSE_WDMA1_PORT ||
-		   sp == PSE_WDMA2_PORT);
+	bool wifi_rx = !!(idev->ieee80211_ptr);
 
 	if ((dsa_port >= 0 && dsa_port <= 4) ||
 	    (dsa_port == 5 && wifi_rx))
diff -urN a/drivers/net/ethernet/mediatek/mtk_ppe.c b/drivers/net/ethernet/mediatek/mtk_ppe.c
--- a/drivers/net/ethernet/mediatek/mtk_ppe.c	2025-11-15 05:48:31.000000000 +0000
+++ b/drivers/net/ethernet/mediatek/mtk_ppe.c	2025-11-25 04:32:07.135732868 +0000
@@ -527,20 +527,29 @@
 }
 
 int mtk_foe_entry_set_dsa(struct mtk_eth *eth, struct mtk_foe_entry *entry,
-			  int port)
+			  int proto, int port)
 {
-	struct mtk_foe_mac_info *l2 = mtk_foe_entry_l2(eth, entry);
+#if IS_ENABLED(CONFIG_NET_DSA)
+	struct mtk_foe_mac_info *l2;
 
-	l2->etype = BIT(port);
+	if (proto == DSA_TAG_PROTO_MXL862_8021Q) {
+		mtk_foe_entry_set_vlan(eth, entry, port + GENMASK(11, 10));
+	} else {
+		l2 = mtk_foe_entry_l2(eth, entry);
+		l2->etype = BIT(port);
 
-	if (!(entry->ib1 & mtk_get_ib1_vlan_layer_mask(eth)))
-		entry->ib1 |= mtk_prep_ib1_vlan_layer(eth, 1);
-	else
-		l2->etype |= BIT(8);
+		if (!(entry->ib1 & mtk_get_ib1_vlan_layer_mask(eth)))
+			entry->ib1 |= mtk_prep_ib1_vlan_layer(eth, 1);
+		else
+			l2->etype |= BIT(8);
 
-	entry->ib1 &= ~mtk_get_ib1_vlan_tag_mask(eth);
+		entry->ib1 &= ~mtk_get_ib1_vlan_tag_mask(eth);
+	}
 
 	return 0;
+#else
+	return -ENOTSUPP;
+#endif
 }
 
 int mtk_foe_entry_set_dscp(struct mtk_eth *eth, struct mtk_foe_entry *entry,
@@ -682,8 +691,7 @@
 		mtk_foe_entry_set_queue(eth, entry, queue + 6);
 }
 
-static int
-mtk_flow_entry_match_len(struct mtk_eth *eth, struct mtk_foe_entry *entry)
+int mtk_flow_entry_match_len(struct mtk_eth *eth, struct mtk_foe_entry *entry)
 {
 	int type = mtk_get_ib1_pkt_type(eth, entry->ib1);
 
@@ -693,8 +701,7 @@
 		return offsetof(struct mtk_foe_entry, ipv4.ib2);
 }
 
-static bool
-mtk_flow_entry_match(struct mtk_eth *eth, struct mtk_flow_entry *entry,
+bool mtk_flow_entry_match(struct mtk_eth *eth, struct mtk_flow_entry *entry,
 		     struct mtk_foe_entry *data, int len)
 {
 	if ((data->ib1 ^ entry->data.ib1) & MTK_FOE_IB1_UDP)
diff -urN a/drivers/net/ethernet/mediatek/mtk_ppe.h b/drivers/net/ethernet/mediatek/mtk_ppe.h
--- a/drivers/net/ethernet/mediatek/mtk_ppe.h	2025-11-15 06:15:29.000000000 +0000
+++ b/drivers/net/ethernet/mediatek/mtk_ppe.h	2025-11-25 03:27:56.538480428 +0000
@@ -394,7 +394,7 @@
 				 __be32 *src_addr, __be16 src_port,
 				 __be32 *dest_addr, __be16 dest_port);
 int mtk_foe_entry_set_dsa(struct mtk_eth *eth, struct mtk_foe_entry *entry,
-			  int port);
+			  int proto, int port);
 int mtk_foe_entry_set_dscp(struct mtk_eth *eth, struct mtk_foe_entry *entry,
 			   unsigned int dscp);
 int mtk_foe_entry_set_vlan(struct mtk_eth *eth, struct mtk_foe_entry *entry,
@@ -413,5 +413,8 @@
 int mtk_ppe_internal_debugfs_init(struct mtk_eth *eth);
 void mtk_foe_entry_get_stats(struct mtk_ppe *ppe, struct mtk_flow_entry *entry,
 			     int *idle);
+int mtk_flow_entry_match_len(struct mtk_eth *eth, struct mtk_foe_entry *entry);
+bool mtk_flow_entry_match(struct mtk_eth *eth, struct mtk_flow_entry *entry,
+			  struct mtk_foe_entry *data, int len);
 
 #endif
diff -urN a/drivers/net/ethernet/mediatek/mtk_ppe_offload.c b/drivers/net/ethernet/mediatek/mtk_ppe_offload.c
--- a/drivers/net/ethernet/mediatek/mtk_ppe_offload.c	2025-11-15 04:06:30.000000000 +0000
+++ b/drivers/net/ethernet/mediatek/mtk_ppe_offload.c	2025-11-25 05:11:24.000000000 +0000
@@ -9,7 +9,7 @@
 #include <linux/ipv6.h>
 #include <net/flow_offload.h>
 #include <net/pkt_cls.h>
-#include <net/netfilter/nf_conntrack.h>
+#include <net/netfilter/nf_conntrack_acct.h>
 #include <net/netfilter/nf_flow_table.h>
 #include <net/dsa.h>
 #include "mtk_eth_soc.h"
@@ -169,7 +169,7 @@
 }
 
 static int
-mtk_flow_get_dsa_port(struct net_device **dev)
+mtk_flow_get_dsa_port(struct net_device **dev, int *proto)
 {
 #if IS_ENABLED(CONFIG_NET_DSA)
 	struct dsa_port *dp;
@@ -178,11 +178,15 @@
 	if (IS_ERR(dp))
 		return -ENODEV;
 
-	if (dp->cpu_dp->tag_ops->proto != DSA_TAG_PROTO_MTK)
+	if (dp->cpu_dp->tag_ops->proto != DSA_TAG_PROTO_MTK &&
+	    dp->cpu_dp->tag_ops->proto != DSA_TAG_PROTO_MXL862_8021Q)
 		return -ENODEV;
 
 	*dev = dsa_port_to_master(dp);
 
+	if (proto)
+		*proto = dp->cpu_dp->tag_ops->proto;
+
 	return dp->index;
 #else
 	return -ENODEV;
@@ -190,17 +194,103 @@
 }
 
 static int
+mtk_flow_get_ct_dir(struct mtk_eth *eth, struct mtk_foe_entry *foe,
+		    struct nf_conn *ct)
+{
+	const struct nf_conntrack_tuple *tuple;
+	struct mtk_flow_entry ct_entry;
+	struct mtk_foe_entry *ct_foe;
+	int i, j, len;
+
+	if (!eth || !foe || !ct)
+		return -EINVAL;
+
+	len = mtk_flow_entry_match_len(eth, foe);
+	ct_foe = &ct_entry.data;
+	ct_foe->ib1 = foe->ib1;
+
+	for (i = 0; i < IP_CT_DIR_MAX; i++) {
+		tuple = &ct->tuplehash[i].tuple;
+
+		if (mtk_get_ib1_pkt_type(eth, foe->ib1) > MTK_PPE_PKT_TYPE_IPV4_DSLITE) {
+			if (nf_ct_l3num(ct) != AF_INET6)
+				return -EINVAL;
+
+			for (j = 0; j < 4; j++) {
+				ct_foe->ipv6.src_ip[j] = ntohl(tuple->src.u3.in6.s6_addr32[j]);
+				ct_foe->ipv6.dest_ip[j] = ntohl(tuple->dst.u3.in6.s6_addr32[j]);
+			}
+			ct_foe->ipv6.src_port = ntohs(tuple->src.u.tcp.port);
+			ct_foe->ipv6.dest_port = ntohs( tuple->dst.u.tcp.port);
+		} else {
+			if (nf_ct_l3num(ct) != AF_INET)
+				return -EINVAL;
+
+			ct_foe->ipv4.orig.src_ip = ntohl(tuple->src.u3.ip);
+			ct_foe->ipv4.orig.dest_ip = ntohl(tuple->dst.u3.ip);
+			ct_foe->ipv4.orig.src_port = ntohs(tuple->src.u.tcp.port);
+			ct_foe->ipv4.orig.dest_port = ntohs(tuple->dst.u.tcp.port);
+		}
+
+		if (mtk_flow_entry_match(eth, &ct_entry, foe, len))
+			return i;
+	}
+
+	return -EINVAL;
+}
+
+static bool
+mtk_flow_is_tcp_ack(struct mtk_eth *eth, struct mtk_foe_entry *foe,
+		    struct nf_conn *ct)
+{
+	const struct nf_conn_counter *counters;
+	struct nf_conn_acct *acct;
+	u64 packets[IP_CT_DIR_MAX], bytes[IP_CT_DIR_MAX], avg[IP_CT_DIR_MAX];
+	int dir;
+
+	if (!ct)
+		return false;
+
+	dir = mtk_flow_get_ct_dir(eth, foe, ct);
+	if (dir < 0 || dir >= IP_CT_DIR_MAX)
+		return false;
+
+	acct = nf_conn_acct_find(ct);
+	if (!acct)
+		return false;
+
+	counters = acct->counter;
+	packets[dir] = atomic64_read(&counters[dir].packets);
+	bytes[dir] = atomic64_read(&counters[dir].bytes);
+	packets[!dir] = atomic64_read(&counters[!dir].packets);
+	bytes[!dir] = atomic64_read(&counters[!dir].bytes);
+
+	/* Avoid division by zero */
+	if (!packets[dir] || !packets[!dir])
+		return false;
+
+	avg[dir] = bytes[dir] / packets[dir];
+	avg[!dir] = bytes[!dir] / packets[!dir];
+
+	/* TCP ACKs are small packets (<= 64 bytes) compared to data packets */
+	return (avg[dir] <= 64 && avg[dir] < avg[!dir]);
+}
+
+static int
 mtk_flow_set_output_device(struct mtk_eth *eth, struct mtk_foe_entry *foe,
-			   struct net_device *dev, struct nf_conn *ct, const u8 *dest_mac,
+			   struct net_device *idev, struct net_device *odev,
+			   struct flow_cls_offload *f, const u8 *dest_mac,
 			   int *wed_index, int dscp)
 {
 	struct mtk_wdma_info info = {};
+	struct nf_conn *ct = NULL;
 	struct mtk_mac *mac;
-	int pse_port, dsa_port, queue;
+	u32 ct_mark = 0;
+	int pse_port, dsa_port, dsa_proto, queue;
 
 	info.tid = dscp;
 
-	if (mtk_flow_get_wdma_info(dev, dest_mac, &info) == 0) {
+	if (mtk_flow_get_wdma_info(odev, dest_mac, &info) == 0) {
 		mtk_foe_entry_set_wdma(eth, foe, info.wdma_idx, info.queue,
 				       info.bss, info.wcid, info.tid, info.amsdu);
 		if (mtk_is_netsys_v2_or_greater(eth)) {
@@ -224,44 +314,56 @@
 		goto out;
 	}
 
-	dsa_port = mtk_flow_get_dsa_port(&dev);
+	dsa_port = mtk_flow_get_dsa_port(&odev, &dsa_proto);
 
-	if (dev == eth->netdev[0]) {
+	if (odev == eth->netdev[0]) {
 		mac = eth->mac[0];
 		pse_port = PSE_GDM1_PORT;
-	} else if (dev == eth->netdev[1]) {
+	} else if (odev == eth->netdev[1]) {
 		mac = eth->mac[1];
 		pse_port = PSE_GDM2_PORT;
-	} else if (dev == eth->netdev[2]) {
+	} else if (odev == eth->netdev[2]) {
 		mac = eth->mac[2];
 		pse_port = PSE_GDM3_PORT;
 	} else
 		return -EOPNOTSUPP;
 
 	if (dsa_port >= 0) {
-		mtk_foe_entry_set_dsa(eth, foe, dsa_port);
-		queue = 3 + dsa_port;
+		mtk_foe_entry_set_dsa(eth, foe, dsa_proto, dsa_port);
+		queue = MTK_MAX_DEVS + eth->pppq_ofs[mac->id] + mac->dsa_user_idx[dsa_port];
 	} else {
 		queue = (pse_port == PSE_GDM3_PORT) ? 2 : pse_port - 1;
 	}
 
-	if ((eth->qos_toggle == 2 || eth->qos_toggle == 3) && mtk_ppe_check_pppq_path(mac, foe, dsa_port))
+	if (f->flow && f->flow->ct) {
+		ct = f->flow->ct;
+		ct_mark = ct->mark;
+	}
+
+	if ((eth->qos_toggle == 2 || eth->qos_toggle == 3) && mtk_ppe_check_pppq_path(mac, idev, dsa_port)) {
+		if ((dsa_port >= 0) && ct && nf_ct_protonum(ct) == IPPROTO_TCP) {
+			/* Dispatch the IPv4/IPv6 TCP Ack packets to the high-priority
+			 * queue, assuming they are less than 64 bytes.
+			 */
+			if (mtk_flow_is_tcp_ack(eth, foe, ct))
+				queue += eth->pppq_ofs[MTK_MAX_DEVS];
+		}
 		mtk_foe_entry_set_queue(eth, foe, queue);
-	else if (eth->qos_toggle == 1 || (ct->mark & MTK_QDMA_QUEUE_MASK) >= 15) {
+	} else if (eth->qos_toggle == 1 || (ct_mark & MTK_QDMA_QUEUE_MASK) >= 15) {
 		u8 qos_ul_toggle;
 
 		if (eth->qos_toggle == 2)
-			qos_ul_toggle = ((ct->mark >> 16) & MTK_QDMA_QUEUE_MASK) >= 15 ? 1 : 0;
+			qos_ul_toggle = ((ct_mark >> 16) & MTK_QDMA_QUEUE_MASK) >= 15 ? 1 : 0;
 		else
-			qos_ul_toggle = ((ct->mark >> 16) & MTK_QDMA_QUEUE_MASK) >= 1 ? 1 : 0;
+			qos_ul_toggle = ((ct_mark >> 16) & MTK_QDMA_QUEUE_MASK) >= 1 ? 1 : 0;
 
 		if (qos_ul_toggle == 1) {
-			if (dev == eth->netdev[1])
-				mtk_foe_entry_set_queue(eth, foe, (ct->mark >> 16) & MTK_QDMA_QUEUE_MASK);
+			if (odev == eth->netdev[1])
+				mtk_foe_entry_set_queue(eth, foe, (ct_mark >> 16) & MTK_QDMA_QUEUE_MASK);
 			else
-				mtk_foe_entry_set_queue(eth, foe, ct->mark & MTK_QDMA_QUEUE_MASK);
+				mtk_foe_entry_set_queue(eth, foe, ct_mark & MTK_QDMA_QUEUE_MASK);
 		} else
-			mtk_foe_entry_set_queue(eth, foe, ct->mark & MTK_QDMA_QUEUE_MASK);
+			mtk_foe_entry_set_queue(eth, foe, ct_mark & MTK_QDMA_QUEUE_MASK);
 	}
 
 out:
@@ -298,6 +400,7 @@
 		flow_rule_match_meta(rule, &match);
 		if (mtk_is_netsys_v2_or_greater(eth)) {
 			idev = __dev_get_by_index(&init_net, match.key->ingress_ifindex);
+			mtk_flow_get_dsa_port(&idev, NULL);
 			if (idev && idev->netdev_ops == eth->netdev[0]->netdev_ops) {
 				struct mtk_mac *mac = netdev_priv(idev);
 
@@ -334,8 +437,6 @@
 
 		flow_rule_match_ip(rule, &match);
 		dscp = match.key->tos;
-	} else {
-		return -EOPNOTSUPP;
 	}
 
 	switch (addr_type) {
@@ -485,6 +586,14 @@
 			return err;
 	}
 
+	err = mtk_flow_set_output_device(eth, &foe, idev, odev, f,
+					 data.eth.h_dest, &wed_index, dscp);
+	if (err)
+		return err;
+
+	if (wed_index >= 0 && (err = mtk_wed_flow_add(wed_index)) < 0)
+		return err;
+
 	if (offload_type == MTK_PPE_PKT_TYPE_BRIDGE)
 		foe.bridge.vlan = data.vlan_in;
 
@@ -494,16 +603,6 @@
 	if (data.pppoe.num == 1)
 		mtk_foe_entry_set_pppoe(eth, &foe, data.pppoe.sid);
 
-	mtk_foe_entry_set_sp(eth->ppe[ppe_index], &foe);
-
-	err = mtk_flow_set_output_device(eth, &foe, odev, f->flow->ct, data.eth.h_dest,
-					 &wed_index, dscp);
-	if (err)
-		return err;
-
-	if (wed_index >= 0 && (err = mtk_wed_flow_add(wed_index)) < 0)
-		return err;
-
 	entry = kzalloc(sizeof(*entry), GFP_KERNEL);
 	if (!entry)
 		return -ENOMEM;
